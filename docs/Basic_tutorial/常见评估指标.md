# 一般问题评价指标

什么是评估指标：

>评估指标即是我们对于一个模型效果的数值型量化。（有点类似与对于一个商品评价打分，而这是针对于模型效果和理想效果之间的一个打分）

一般来说分类和回归问题的评价指标有如下一些形式：

## 分类算法常见的评估指标如下：

* 对于二类分类器/分类算法，评价指标主要有accuracy， [Precision，Recall，F-score，Pr曲线]，ROC-AUC曲线。
* 对于多类分类器/分类算法，评价指标主要有accuracy， [宏平均和微平均，F-score]。

## 对于回归预测类常见的评估指标如下:

* 平均绝对误差（Mean Absolute Error，MAE），均方误差（Mean Squared Error，MSE），平均绝对百分误差（Mean Absolute Percentage Error，MAPE），均方根误差（Root Mean Squared Error）， R2（R-Square）

下面进行详细介绍：

1. **平均绝对误差（Mean Absolute Error，MAE）**，能更好地反映预测值与真实值误差的实际情况：

$$
MAE=\frac{1}{N} \sum_{i=1}^{N}\left|y_{i}-\hat{y}_{i}\right|
$$



2. **均方误差（Mean Squared Error，MSE）**：

$$
MSE=\frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}
$$

3. RMSE(Root Mean Squard Error)

$$
RMSE(y,f(x))=\frac{1}{1+MSE(y,f(x))}
$$

4. **R2（R-Square）的公式为**：

   残差平方和：
   $$
   SS_{res}=\sum\left(y_{i}-\hat{y}_{i}\right)^{2}
   $$
   
   总平均值:
   
   $$
   SS_{tot}=\sum\left(y_{i}-\overline{y}_{i}\right)^{2}
   $$
   
   其中$\overline{y}$表示$y$的平均值,得到$R^2$表达式为：
   $$
   R^{2}=1-\frac{SS_{res}}{SS_{tot}}=1-\frac{\sum\left(y_{i}-\hat{y}_{i}\right)^{2}}{\sum\left(y_{i}-\overline{y}\right)^{2}}
   $$
   $R^2$用于度量因变量的变异中可由自变量解释部分所占的比例，取值范围是 0~1，$R^2$越接近1,表明回归平方和占总平方和的比例越大,回归线与各观测点越接近，用x的变化来解释y值变化的部分就越多,回归的拟合程度就越好。所以$R^2$也称为拟合优度（Goodness of Fit）的统计量。
   
   $y_{i}$表示真实值，$\hat{y}_{i}$表示预测值，$\overline{y}_{i}$表示样本均值。得分越高拟合效果越好。

5. 混淆矩阵

   | 混淆矩阵            | Predicted as Positive | Predicted as Negative |
   | ------------------- | --------------------- | --------------------- |
   | Labeled as Positive | True Positive(TP)     | False Negative(FN)    |
   | Labeled as Negative | False Positive(FP)    | True Negative(TN)     |

* 真正例(True Positive, TP):真实类别为正例, 预测类别为正例
* 假负例(False Negative, FN): 真实类别为正例, 预测类别为负例
* 假正例(False Positive, FP): 真实类别为负例, 预测类别为正例 
* 真负例(True Negative, TN): 真实类别为负例, 预测类别为负例

* 真正率(True Positive Rate, TPR): 被预测为正的正样本数 / 正样本实际数

$$
TPR=\frac{TP}{TP+FN}
$$

* 假负率(False Negative Rate, FNR): 被预测为负的正样本数/正样本实际数

$$
FNR=\frac{FN}{TP+FN}
$$

* 假正率(False Positive Rate, FPR): 被预测为正的负样本数/负样本实际数，

$$
FPR=\frac{FP}{FP+TN}
$$

* 真负率(True Negative Rate, TNR): 被预测为负的负样本数/负样本实际数，

$$
TNR=\frac{TN}{FP+TN}
$$

* 准确率（Accuracy）

$$
ACC=\frac{TP+TN}{TP+FN+FP+TN}
$$

* 精准率（Precision）

$$
P=\frac{TP}{TP+FP}
$$

* 召回率（Recall）

$$
R=\frac{TP}{TP+FN}
$$

* F1-Score

$$
\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}
$$

* ROC

  ROC曲线的横轴为“假正例率”，纵轴为“真正例率”. 以FPR为横坐标，TPR为纵坐标，那么ROC曲线就是改变各种阈值后得到的所有坐标点 (FPR,TPR) 的连线，画出来如下。红线是随机乱猜情况下的ROC，曲线越靠左上角，分类器越佳. 

* AUC(Area Under Curve)

  AUC就是ROC曲线下的面积. 真实情况下，由于数据是一个一个的，阈值被离散化，呈现的曲线便是锯齿状的，当然数据越多，阈值分的越细，”曲线”越光滑. 

  图片

  用AUC判断分类器（预测模型）优劣的标准:

  + AUC = 1 是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器.

  + 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值.

  + AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测.

6. **交叉验证**

   交叉验证（Cross Validation）是一种比较好的衡量机器学习模型的统计分析方法，可以有效避免划分训练集和测试集时的随机性对评价结果造成的影响。我们可以把原始数据集平均分为$K$组不重复的子集，每次选$K − 1$组子集（ $K$一般大于3）作为训练集，剩下的一组子集作为验证集。这样可以进行$K$ 次试验并得到$K$ 个模型，将这$K$ 个模型在各自验证集上的错误率的平均作为分类器的评价。

https://www.cnblogs.com/jin-liang/p/9539622.html

------

# 目标检测评价指标

AP (Average precision)是主流的目标检测模型的评价指标。再介绍AP之前，我们先来回顾一下需要用到的几个概念precision，recall以及IoU。

## IoU（**Intersection over union**）

交并比IoU衡量的是两个区域的重叠程度，是两个区域重叠部分面积占二者总面积（重叠部分只计算一次）的比例。如下图，两个矩形框的IoU是交叉面积（中间图片红色部分）与合并面积（右图红色部分）面积之比。

![img](https://pic1.zhimg.com/80/v2-2df5d99d172d37abd930ab7544124dc9_720w.jpg)Iou的定义

在目标检测任务中，如果我们**模型输出的矩形框与我们人工标注的矩形框的IoU值大于某个阈值时（通常为0.5）即认为我们的模型输出了正确的**

## **精准率与召回率（Precision & Recall）**

Precision 和 Recall最早是信息检索中的概念，用来评价一个信息检索系统的优劣。Precision 就是检索出来的条目中（比如：文档、网页等）有多大比例是我们需要的，Recall就是所有我们需要的网页的条目有多大比例被检索出来了。在目标检测领域，假设我们有一组图片，里面有若干待检测的目标，Precision就代表我们模型检测出来的目标有多打比例是真正的目标物体，Recall就代表所有真实的目标有多大比例被我们的模型检测出来了。

具体如何计算Precision和Recall的值在前面已经讲过了，此处不再细讲

## PR曲线

我们当然希望检测的结果P越高越好，R也越高越好，但事实上这两者在**某些情况下是矛盾的**。比如极端情况下，我们只检测出了一个结果，且是准确的，那么Precision就是100%，但是Recall就很低；而如果我们把所有结果都返回，那么必然Recall必然很大，但是Precision很低。

因此在不同的场合中需要自己判断希望P比较高还是R比较高。如果是做实验研究，可以绘制Precision-Recall曲线来帮助分析。

这里我们举一个简单的例子，假设我们的数据集中共有五个待检测的物体，我们的模型给出了10个候选框，我们按照模型给出的置信度由高到低对候选框进行排序。

![img](https://pic3.zhimg.com/80/v2-866d3e50348f6f882a9f4a81d9f641e2_720w.jpg)

表格第二列表示该候选框是否预测正确（即是否存在某个待检测的物体与该候选框的iou值大于0.5）第三列和第四列表示以该行所在候选框置信度为阈值时，Precision和Recall的值。我们以表格的第三行为例进行计算：
$$
TP=2，FP=1，TN=3
$$

$$
Precision=\frac{2}{2+1}=0.67
$$

$$
Recall=\frac{2}{2+3}=0.4
$$

由上表以Recall值为横轴，Precision值为纵轴，我们就可以得到PR曲线。我们会发现，Precision与Recall的值呈现负相关，在局部区域会上下波动。

![img](https://picb.zhimg.com/80/v2-8e5dea717aa368da9e66d91f4fbe053c_720w.jpg)

<center><br>PR曲线</br></center>

------

## AP(Average Precision)

顾名思义AP就是平均精准度，简单来说就是对**PR曲线上的Precision值求均值**，不是每一张图片求一次Precision然后求和求平均。对于pr曲线来说，我们使用积分来进行计算。
$$
AP=\int_0^1p(r)dr
$$
在实际应用中，我们并不直接对该PR曲线进行计算，而是对PR曲线进行**平滑处理**。即对PR曲线上的每个点，Precision的值取该点右侧最大的Precision的值。

![img](https://pic2.zhimg.com/80/v2-1874dda585ec6eb90570889c8efeeaad_720w.jpg)

<center><br>PR曲线的平滑处理</br></center>

用公式来描述就是 $P_{smooth}(r)= \substack{\displaystyle\max\\r'>=r} P(r')$ 。用该公式进行平滑后再用上述公式计算AP的值。

------

## Interplolated AP（Pascal Voc 2008 的AP计算方式）

Pascal VOC 2008中设置IoU的阈值为0.5，如果一个目标被重复检测，则置信度最高的为正样本，另一个为负样本。在平滑处理的PR曲线上，取横轴0-1的10等分点（包括断点共11个点）的Precision的值，计算其平均值为最终AP的值。
$$
AP=\frac{1}{11}\sum_{0,0.1\dots1.0}P_{smooth}(i)
$$
![img](https://pic3.zhimg.com/80/v2-4b48d55c7bbb87a0c98e50803a0e519d_720w.jpg)

<center><br>Pascal Voc 2008 AP计算方式</br></center>

在我们的例子里
$$
AP=\frac{1}{11}(5\times1+4\times0.57+2\times0.5)=0.753
$$

------

## Area under curve

上述方法有两个缺陷，第一个是使用11个采样点在精度方面会有损失。第二个是，在比较两个AP值较小的模型时，很难体现出两者的差别。所以这种方法在2009年的Pascalvoc之后便不再采用了。在Pascal voc 2010之后，便开始采用这种精度更高的方式。绘制出平滑后的PR曲线后，用积分的方式计算平滑曲线下方的面积作为最终的AP值。
$$
AP=\int_0^1p_{smooth}(r)dr
$$
![img](https://pic3.zhimg.com/80/v2-85f806880f123a9b4c523ba59dc04fe9_720w.jpg)

<center><br>Pascal voc 2010-2012 AP 计算方式</br></center>

------

## COCO mAP

最新的目标检测相关论文都使用coco数据集来展示自己模型的效果。对于coco数据集来说，使用的也是Interplolated AP的计算方式。与Voc 2008不同的是，为了提高精度，在PR曲线上采样了100个点进行计算。而且Iou的阈值从固定的0.5调整为在 0.5 - 0.95 的区间上每隔0.05计算一次AP的值，取所有结果的平均值作为最终的结果。

比如我们看一下YOLOv3的作者在论文中展示的在coco数据集上的实验结果

![img](https://pic3.zhimg.com/80/v2-c30898ad97f21ddac679afdec0d10d53_720w.jpg)

我们发现除了$AP$，还有$AP_{50},AP_{75}$等值，这些事代表什么意思呢？

$AP_{50}$：IoU阈值为0.5时的AP测量值

$AP_{75}$：IoU阈值为0.75时的测量值

$AP_S$: 像素面积小于 ${32}^2$ 的目标框的AP测量值

$AP_M$ : 像素面积在${32}^2-{96}^2$- 之间目标框的测量值

$AP_L$ : 像素面积大于 ${96}^2$ 的目标框的AP测量值

注：通常来说AP是在单个类别下的，mAP是AP值在**所有类别下的均值**。在这里，在coco的语境下AP便是mAP，这里的AP已经计算了所有类别下的平均值，这里的AP便是mAP。