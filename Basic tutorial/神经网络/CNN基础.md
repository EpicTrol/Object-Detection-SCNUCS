## 卷积（Convolution）、填充（Padding）、步长(Stride)

![CNN基础知识——卷积（Convolution）、填充（Padding）、步长(Stride)](https://pic3.zhimg.com/v2-e4926fc92ba050940b80d1aead522b33_r.jpg)

近几年来，在深度学习领域，**”卷积神经网络“**一度成为大家的”宠儿“，深受大众青睐。

**卷积神经网络**（convolutional neural network，CNN）是指至少在网络的一层中**使用卷积运算来代替一般的矩阵乘法运算**的神经网络，因此命名为卷积神经网络。那什么是卷积运算啊？接下来我们一起来揭开它神秘的面纱。

**【卷积（Convolution）】**

我们以灰度图像为例进行讲解：从一个小小的权重矩阵，也就是卷积核（kernel）开始，让它逐步在二维输入数据上“扫描”。卷积核“滑动”的同时，计算权重矩阵和扫描所得的数据矩阵的乘积，然后把结果汇总成一个输出像素。

![img](https://pic3.zhimg.com/v2-6428cf505ac1e9e1cf462e1ec8fe9a68_b.webp)

![](https://pic2.zhimg.com/v2-705305fee5a050575544c64067405fce_b.webp)

深度学习里面所谓的卷积运算，其实它被称为**互相关（cross-correlation）运算：**将图像矩阵中，从左到右，由上到下，取与滤波器同等大小的一部分，每一部分中的值与滤波器中的值对应相乘后求和，最后的结果组成一个矩阵，其中没有对核进行翻转。

**【填充（Padding）】**

前面可以发现，输入图像与卷积核进行卷积后的结果中损失了部分值，输入图像的边缘被“修剪”掉了（边缘处只检测了部分像素点，丢失了图片边界处的众多信息）。这是因为边缘上的像素永远不会位于卷积核中心，而卷积核也没法扩展到边缘区域以外。这个结果我们是不能接受的，有时我们还希望输入和输出的大小应该保持一致。为解决这个问题，可以在进行卷积操作前，对原矩阵进行边界**填充（Padding）**，也就是在矩阵的边界上填充一些值，以增加矩阵的大小，通常都用“$0$”来进行填充的。

![img](https://picb.zhimg.com/v2-2a2307d5c20551f1a3e8458c7070cf16_b.webp)

通过填充的方法，当卷积核扫描输入数据时，它能延伸到边缘以外的伪像素，从而使输出和输入size相同。

常用的两种padding：

**（1）valid padding**：不进行任何处理，只使用原始图像，不允许卷积核超出原始图像边界

**（2）same padding**：进行填充，允许卷积核超出原始图像边界，并使得卷积后结果的大小与原来的一致

**【步长(Stride)】**

滑动卷积核时，我们会先从输入的左上角开始，每次往左滑动一列或者往下滑动一行逐一计算输出，我们将每次滑动的行数和列数称为Stride，在之前的图片中，Stride=1；在下图中，Stride=2。

![img](https://picb.zhimg.com/v2-294159b043a917ea622e1794b4857a34_b.webp)

卷积过程中，有时需要通过padding来避免信息损失，有时也要在卷积时通过设置的**步长（Stride）**来压缩一部分信息，或者使输出的尺寸小于输入的尺寸。

![img](https://pic1.zhimg.com/v2-c14af9d136b1431018146118492b0856_b.webp)**Stride的作用：**是成倍缩小尺寸，而这个参数的值就是缩小的具体倍数，比如步幅为2，输出就是输入的1/2；步幅为3，输出就是输入的1/3。以此类推。

**【卷积核的大小一般为奇数\*奇数】** $1*1$，$3*3$，$5*5$，$7*7$都是最常见的。**这是为什么呢？**为什么没有偶数*偶数​？

**（1）更容易padding**

在卷积时，我们有时候需要卷积前后的尺寸不变。这时候我们就需要用到padding。假设图像的大小，也就是被卷积对象的大小为$n*n$，卷积核大小为$k*k$，padding的幅度设为$(k-1)/2$时，卷积后的输出就为$(n-k+2*((k-1)/2))/1+1=n$，即卷积输出为$n*n$，保证了卷积前后尺寸不变。但是如果$k$是偶数的话，$(k-1)/2$就不是整数了。

**（2）更容易找到卷积锚点**

在CNN中，进行卷积操作时一般会以卷积核模块的一个位置为基准进行滑动，这个基准通常就是卷积核模块的中心。若卷积核为奇数，卷积锚点很好找，自然就是卷积模块中心，但如果卷积核是偶数，这时候就没有办法确定了，让谁是锚点似乎都不怎么好。

**【卷积的计算公式】**

**输入图片的尺寸：**一般用 $n * n$ 表示输入的image大小。**卷积核的大小：**一般用 $f*f$ 表示卷积核的大小。

**填充（Padding）：**一般用 $p$来表示填充大小。

**步长(Stride)：**一般用 $s$ 来表示步长大小。

**输出图片的尺寸：**一般用 $o$ 来表示。如果已知  $n$、 $f$ 、  、 $p$、$s$ 可以求得 $o$ ，**计算公式如下：**$o= \lfloor \frac{n+2p-f}{s} \rfloor +1$其中"$\lfloor $ $  \rfloor$"是向下取整符号，用于结果不是整数时进行向下取整。

**【多通道卷积】**上述例子都只包含一个输入通道。实际上，大多数输入图像都有 RGB 3个通道。![img](https://pic1.zhimg.com/80/v2-fc70463d7f82f7268ee23b7235515f4a_720w.jpg)这里就要涉及到“卷积核”和“filter”这两个术语的区别。在只有一个通道的情况下，“卷积核”就相当于“filter”，这两个概念是可以互换的。但在一般情况下，它们是两个完全不同的概念。**每个“filter”实际上恰好是“卷积核”的一个集合**，在当前层，每个通道都对应一个卷积核，且这个卷积核是独一无二的。

**多通道卷积的计算过程：**将矩阵与滤波器对应的每一个通道进行卷积运算，最后相加，形成一个单通道输出，加上偏置项后，我们得到了一个最终的单通道输出。如果存在多个filter，这时我们可以把这些最终的单通道输出组合成一个总输出。这里我们还需要**注意**一些问题——滤波器的通道数、输出特征图的通道数。

**某一层滤波器的通道数 = 上一层特征图的通道数。**如上图所示，我们输入一张 $6\times6\times3$ 的RGB图片，那么滤波器（$3\times3\times3$）也要有三个通道。

**某一层输出特征图的通道数 = 当前层滤波器的个数。**如上图所示，当只有一个filter时，输出特征图（ ）的通道数为1；当有2个filter时，输出特征图（ $4\times4\times2$）的通道数为2。



## 池化（pooling）

池化过程在一般卷积过程后。池化（pooling） 的本质，其实就是采样。Pooling 对于输入的 Feature Map，选择某种方式对其进行降维压缩，以加快运算速度。

采用较多的一种池化过程叫**最大池化（Max Pooling）**，其具体操作过程如下：

![img](https://pic2.zhimg.com/80/v2-2ce695fbd365c2be94521992d52ccefd_720w.jpg)

池化过程类似于卷积过程，如上图所示，表示的就是对一个 $4\times4$ feature map邻域内的值，用一个  $2\times2$的filter，步长为2进行‘扫描’，选择最大值输出到下一层，这叫做 Max Pooling。

max pooling常用的 $s=2,f=2$ ，的效果：特征图高度、宽度减半，通道数不变。

还有一种叫**平均池化（Average Pooling）**,就是从以上取某个区域的最大值改为求这个区域的平均值，其具体操作过程如下：

![img](https://pic2.zhimg.com/80/v2-a47095dd0902990d387e21ae24e6f0b9_720w.jpg)

如上图所示，表示的就是对一个 $4\times4$ feature map邻域内的值，用一个 $2\times2$ 的filter，步长为2进行‘扫描’，计算平均值输出到下一层，这叫做 Mean Pooling。

**【池化层没有参数】**

**池化的作用：**

（1）保留主要特征的同时减少参数和计算量，防止过拟合。

（2）invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。

Pooling 层说到底还是一个特征选择，信息过滤的过程。也就是说我们损失了一部分信息，这是一个和计算性能的一个妥协，随着运算速度的不断提高，我认为这个妥协会越来越小。

现在有些网络都开始少用或者不用pooling层了。



## 全连接层（Fully Connected Layer）

![img](https://picb.zhimg.com/80/v2-528850b9810ba8aed8f7a841db3532ea_720w.jpg)

全连接层就是将最后一层卷积得到的特征图（矩阵）展开成一维向量，并为分类器提供输入。最开始看到这个全连接层，我就很是疑问：它是怎么做的呢？

举个列子：

![img](https://pic4.zhimg.com/80/v2-278687a5273635b6c78e79e854a11696_720w.jpg)

我们输入一个 $28\times28$ 的灰度图像，经过卷积层和池化层输出20个的图像，然后通过了一个全连接层变成了$1\times100$的向量。

这是怎么做到的呢？很简单，**可以理解为在中间做了一次卷积**。我们用一个12x12x20的filter 去卷积激活函数的输出，得到的结果就是一个fully connected layer 的一个神经元的输出，这个输出就是一个值。因为我们有100个神经元，所以输出一个$1\times100$ 的向量。**我们实际就是用一个$12\times12\times20\times100$的卷积层去卷积激活函数的输出，最终得到 $1\times100$的向量。**

**【全连接层的作用】：**全连接层在整个网络卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数等操作是将原始数据映射到隐层特征空间的话（特征提取+选择的过程），**全连接层则起到将学到的特征表示映射到样本的标记空间的作用**。换句话说，**就是把特征整合到一起**（高度提纯特征）**，方便交给最后的分类器或者回归。**

**【全连接层存在的问题】：**参数冗余（仅全连接层参数就可占整个网络参数80%左右），降低了训练的速度，容易过拟合。

**CNN（带有FC层）输入图片尺寸是固定的原因：**全连接层要求固定的输入维度（如4096）

**CNN支持任意尺寸输入图像的方法：**（1）使用全局平均池化层或卷积层替换FC层（2）在卷积层和FC层之间加入空间金字塔池化



另一个参考：https://blog.csdn.net/quiet_girl/article/details/84579038