# 引言

在神经网络中，为了更有效的计算梯度，需要用到反向传播算法。我们先从链式求导法则开始。

# 链式求导法

先介绍下链式求导法则，在后面的反向传播算法中会用到。

有$y=g(x),z=h(y)$

那么$\frac{dz}{dx} = \frac{dz}{dy}\frac{dy}{dx}$;

有$x=g(s),y=h(s),z=k(x,y)$

![](https://img-blog.csdnimg.cn/20191226221356609.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqdzEyMzQ1Ng==,size_16,color_FFFFFF,t_70)

改变了s会改变x和y，从而改变了z。

$\frac{dz}{ds} = \frac{\partial{z}}{\partial{x}}\frac{dx}{ds} + \frac{\partial{z}}{\partial{y}}\frac{dy}{ds}$

注意，如果改变s会改变多个变量，它们的关系也是成立的。

# 损失函数

![[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-dnxvSeRt-1577369599078)(_v_images/20191226203608959_23215.png)]](https://img-blog.csdnimg.cn/20191226221453414.png)

假设给定一组参数$\theta$,把一个训练数据$x^n$代入NN(神经网络)中，会得到输出$y^n$。

$C^n$是输出$y^n$和实际$\hat{y}^n$距离函数，值越大代表越距离远，也就是效果越不好。

那在神经网络训练算法中，损失函数定义为：
$$
L(\theta) = \sum^{N}_{n=1}C^n(\theta)
$$
如果把损失函数对参数$w$做微分的话，得到
$$
\frac{\partial L(\theta)}{\partial w} = \sum^{N}_{n=1}\frac{\partial C^n(\theta)}{\partial w}
$$
只要计算出某一笔数据对$w$的微分，就可以得到$L(\theta)$对$w$的微分。

![](https://img-blog.csdnimg.cn/20191226221448713.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqdzEyMzQ1Ng==,size_16,color_FFFFFF,t_70)

假设我们先考虑这个神经元。

![](https://img-blog.csdnimg.cn/20191226221508285.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqdzEyMzQ1Ng==,size_16,color_FFFFFF,t_70)

假设只有两个输入$x_1,x_2$，计算$z= x_1w_1 + x_2w_2 + b$得到$z$后再代入激活函数，经过多次运算会得到最终的输出$y_1,y_2$。

![](https://img-blog.csdnimg.cn/20191226221508636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqdzEyMzQ1Ng==,size_16,color_FFFFFF,t_70)

现在问题是如何计算损失(距离函数)$C$对$w$的偏微分$\frac{\partial C}{\partial w}$

利用链式求导法
$$
\frac{\partial C}{\partial w} = \frac{\partial C}{\partial z} \frac{\partial z}{\partial w}
$$
计算$\frac{\partial z}{\partial w}$的过程叫做**正向过程(Forward pass)**；计算$\frac{\partial C}{\partial z}$的过程叫做**反向过程(Backward pass)**。

# 正向过程

$$z = x_1w_1 + x_2w_2 + b$$
$$
\frac{\partial z}{\partial w_1} = x_1 \\\frac{\partial z}{\partial w_2} = x_2
$$
![](https://img-blog.csdnimg.cn/20191226221524188.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqdzEyMzQ1Ng==,size_16,color_FFFFFF,t_70)



如上图所示，假设输入是$1,−1$，上面蓝色神经元的参数：$w_1=1,w_2=-2,b=1$，激活函数是`Sigmoid`函数；
下面蓝色神经元的参数：$w_1=-1,w_2=1,b=0$

对下面的神经元来说，计算$w_2$的偏微分，可以很快得出$\frac{\partial z}{\partial w} = -1$，也就是输入$x_2(-1)$，随着从前往后计算每个神经元的输出，整个过程就可以很快结束，因此叫正向过程。

# 反向过程

![](https://img-blog.csdnimg.cn/20191226221532561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqdzEyMzQ1Ng==,size_16,color_FFFFFF,t_70)

困难的是如何计算$\frac{\partial C}{\partial z}$

> $a = \frac{1}{1+e^{-z}}$

假设激活函数是`Sigmoid`函数$a=\sigma(z)$，然后得到的函数值aa*a*会乘上某个权重(比如$w_3$)再加上其他值得到$z^\prime$(注意这里只是一个符号，不是$z$的导数)；$a$也会乘上权重(比如$w_4$)再加上其他东西得到$z^{\prime\prime}$(注意这里只是一个符号，不是$z$的二阶导数)；

![](https://img-blog.csdnimg.cn/20191226221541622.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqdzEyMzQ1Ng==,size_16,color_FFFFFF,t_70)
$$
\frac{\partial C}{\partial z} = \frac{\partial C}{\partial a} \frac{\partial a}{\partial z}
$$
可以这样理解，$z$通过影响$a$来影响$C$。

而
$$
\frac{\partial a}{\partial z} = \frac{\partial \sigma(z)}{\partial z} = \sigma^\prime(z)
$$
那就剩下
$$
\frac{\partial C}{\partial a} = \frac{\partial C}{\partial z^\prime}\frac{\partial z^\prime}{\partial a} + \frac{\partial C}{\partial z^{\prime\prime}}\frac{\partial z^{\prime\prime}}{\partial a}
$$
改变了$a$会改变$z^{\prime}$和$z^{\prime\prime}$，从而改变了$C$

我们先计算简单的

z′=aw3+⋯z^{\prime} = aw_3 + \cdots*z*′=*a**w*3+⋯

有
∂z′∂a=w3\frac{\partial z^{\prime}}{\partial a} = w_3∂*a*∂*z*′​=*w*3​

同理

∂z''∂a=w4\frac{\partial z^{\prime\prime}}{\partial a} = w_4∂*a*∂*z*′′=*w*4

现在难点就是∂C∂z′\frac{\partial C}{\partial z^\prime}∂*z*′∂*C*和∂C∂z''\frac{\partial C}{\partial z^{\prime\prime}}∂*z*′′∂*C*

我们这里先假装我们知道这两项的值。然后整理下原来的式子:

∂C∂z=σ′(z)[w3∂C∂z′+w4∂C∂z'']\frac{\partial C}{\partial z} = \sigma^\prime(z)[w_3\frac{\partial C}{\partial z^\prime} + w_4\frac{\partial C}{\partial z^{\prime\prime}}]∂*z*∂*C*=*σ*′(*z*)[*w*3∂*z*′∂*C*+*w*4∂*z*′′∂*C*]

![](https://img-blog.csdnimg.cn/20191226221557811.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqdzEyMzQ1Ng==,size_16,color_FFFFFF,t_70)

假设有另外一个特殊的神经元，它是上图的样子，输入就是∂C∂z′\frac{\partial C}{\partial z^\prime}∂*z*′∂*C*和∂C∂z''\frac{\partial C}{\partial z^{\prime\prime}}∂*z*′′∂*C*，它们分别乘以w3w_3*w*3和w4w_4*w*4，然后求和得到的结果再乘上σ′(z)\sigma^\prime(z)*σ*′(*z*)
就得到了∂C∂z\frac{\partial C}{\partial z}∂*z*∂*C*​

zz*z*在正向传播的过程中已经知道了，因此这里的σ′(z)\sigma^\prime(z)*σ*′(*z*)是一个常数。

说了这么多，还是没说怎么计算∂C∂z′\frac{\partial C}{\partial z^\prime}∂*z*′∂*C*和∂C∂z''\frac{\partial C}{\partial z^{\prime\prime}}∂*z*′′∂*C*啊。别急，下面就开始计算。

这里要分两种情况考虑：

![](https://img-blog.csdnimg.cn/20191226221605604.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqdzEyMzQ1Ng==,size_16,color_FFFFFF,t_70)

情形一： 红色的两个神经元就是输出层，它们能直接得到输出。

根据链式法则有：

$\frac{\partial C}{\partial z^\prime} = \frac{\partial y_1}{\partial z^\prime}\frac{\partial C}{\partial y_1}$

只要知道激活函数是啥就能计算出∂y1∂z′\frac{\partial y_1}{\partial z^\prime}∂*z*′∂*y*1

∂C∂y1\frac{\partial C}{\partial y_1}∂*y*1∂*C*也可以根据我们选取的损失函数简单的计算出来。

同理∂C∂z''\frac{\partial C}{\partial z^{\prime\prime}}∂*z*′′∂*C*的计算也一样

情形二：红色的不是输出层

![](https://img-blog.csdnimg.cn/2019122622161414.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqdzEyMzQ1Ng==,size_16,color_FFFFFF,t_70)

红色的是中间层，它们的激活函数的值会当成下一层的输入继续参数计算。

![[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-UefOrgCj-1577369599081)(_v_images/20191226220203239_4057.png)]](https://img-blog.csdnimg.cn/20191226221621237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqdzEyMzQ1Ng==,size_16,color_FFFFFF,t_70)

如果我们知道∂C∂za\frac{\partial C}{\partial z_a}∂*z**a*∂*C*和∂C∂zb\frac{\partial C}{\partial z_b}∂*z**b*∂*C*

同理(回顾一下上面那个特殊的神经元)我们就可以计算∂C∂z′\frac{\partial C}{\partial z^{\prime}}∂*z*′∂*C*

![[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-p2J9i11V-1577369599081)(_v_images/20191226220512917_26589.png)]](https://img-blog.csdnimg.cn/20191226221627503.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqdzEyMzQ1Ng==,size_16,color_FFFFFF,t_70)

问题就会这样反复循环下去，我们不停的看下一层，直到遇到了输出层。然后就可以由输出层往前计算出整个NN的所有的参数。

那我们为何不换个角度考虑问题，我们直接先算输出层的偏微分，然后依次往前计算。

![[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ORibfTTy-1577369599081)(_v_images/20191226221214641_7228.png)]](https://img-blog.csdnimg.cn/2019122622163668.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqdzEyMzQ1Ng==,size_16,color_FFFFFF,t_70)