## 神经元模型

神经元是神经网络中最基本的结构，也可以说是神经网络的基本单元，它的设计灵感完全来源于生物学上神经元的信息传播机制。我们学过生物的同学都知道，神经元有两种状态：兴奋和抑制。一般情况下，大多数的神经元是处于抑制状态，但是一旦某个神经元收到刺激，导致它的电位超过一个**阈值**，那么这个神经元就会被激活，处于“兴奋”状态，进而向其他的神经元传播化学物质（其实就是信息）。

　　下图为生物学上的神经元结构示意图：

![img](https://images2015.cnblogs.com/blog/764050/201606/764050-20160619111613406-1210494225.png)

1943年，McCulloch和Pitts将上图的神经元结构用一种简单的模型进行了表示，构成了一种人工神经元模型，也就是我们现在经常用到的“M-P神经元模型”，如下图所示：

![img](https://images2015.cnblogs.com/blog/764050/201606/764050-20160619112701960-1012598812.png)

　　从上图M-P神经元模型可以看出，神经元的输出
$$
y = f(\sum_{i=1}^{n}w_{i}x_{i} - \theta)
$$
　　其中$\theta$为我们之前提到的神经元的激活阈值，函数$f(⋅)$也被称为是激活函数。如上图所示，函数$f(⋅)$可以用一个阶跃方程表示，大于阈值激活；否则则抑制。但是这样有点太粗暴，因为阶跃函数不光滑，不连续，不可导，因此我们更常用的方法是用sigmoid函数来表示函数函数$f(⋅)$。

　　sigmoid函数的表达式和分布图如下所示：

![img](https://images2015.cnblogs.com/blog/764050/201606/764050-20160619132616585-890841084.png)
$$
f(x) = \frac{1}{1+e^{-x}}
$$

## 感知机与神经网络

感知机（perceptron）是由两层神经元组成的结构，输入层用于接受外界输入信号，输出层（也被称为是感知机的功能层）就是M-P神经元。下图表示了一个输入层具有三个神经元（分别表示为$x_{0}$、$x_{1}$、$x_{2}$）的感知机结构：

![img](https://images2015.cnblogs.com/blog/764050/201606/764050-20160619145050085-1140057304.jpg)

　　根据上图不难理解，感知机模型可以由如下公式表示：
$$
y = f(wx + b)
$$
其中，$w$为感知机输入层到输出层连接的权重，$b$表示输出层的偏置

## [深入浅出卷积神经网络CNN](https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247499511&idx=1&sn=a420a254f767241e6b3c40e55b28a963&scene=21#wechat_redirect)

![](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsHlsIZib4QU2kdcKN9xdz0XBdAvJibWlY7SvTHLl6mJVRIL5knb6oBmCn13XqG4OcpCD6y4iaZPzrSYQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

卷积神经网络（Convolutional Neural Network, CNN）是一类特殊的人工神经网络，是深度学习中重要的一个分支。在图像分类时，常常面临着图像大，物体的形态、位置不同等问题，这就给普通的神经网络带来了难题，而CNN就是来解决这个问题，它的精度和速度比传统计算学习算法高很多。特别是在计算机视觉领域，CNN是解决图像分类、图像检索、物体检测和语义分割的主流模型。

CNN每一层由众多的卷积核组成，每个卷积核对输入的像素进行卷积操作，得到下一次的输入。随着网络层的增加卷积核会逐渐扩大感受野，并缩减图像的尺寸。

卷积神经网络与普通神经网络非常相似，它们都由具有可学习的权重和偏置常量的神经元组成。每个神经元都接收一些输入，并做一些点积计算，输出是每个分类的分数，普通神经网络里的一些计算技巧到这里依旧适用。

两者的不同点在于，卷积神经网络默认输入是图像，可以让我们把特定的性质编码入网络结构，使我们的前馈函数更加有效率，并减少了大量参数。



这是一篇不错的介绍https://mp.weixin.qq.com/s/Kgt7fyzkcfEClcI_xgvddw



**1. 具有三维体积的神经元(3D volumes of neurons)**

卷积神经网络利用输入图片的特点，把神经元设计成三个维度 ：width, height, depth。比如输入的图片大小是 32 × 32 × 3 (rgb)，那么输入神经元就也具有 32×32×3 的维度。下面是传统神经网络的示意图：

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsFtXNXWGT70mokV6vkgUD9jp8BJjCQSLMX31kCTVIguibekc39w2yKZzwZbmq5Zo01XogmEoBY2BEg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

一个卷积神经网络由很多层组成，它们的输入是三维的，输出也是三维的，有的层有参数，有的层不需要参数。卷积神经网络的示意图如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsFtXNXWGT70mokV6vkgUD9jZuYtpqPaQSKLWoqqhTmicjpzweMTAXruRcLqS6w8iaJqDwe9JyzaCnqQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**2. 卷积神经网络结构**

**2.1 卷积层（Convolutional layer）**

卷积神经网路中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法优化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsFtXNXWGT70mokV6vkgUD9j26mKJOAFxEQuFo07R6ehTAfgaoia4IUWKGMwOXnXGqoxJgjsdGPfXtw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

那如何提前知道卷积核呢？假如是人脸识别这样成千上万个特征的图片，我们就没办法提前知道什么是合适的卷积核。其实也没必要知道，因为选择什么样的卷积核，完全可以通过训练不断优化。初始时只需要随机设置一些卷积核，通过训练，模型其实自己可以学习到合适的卷积核，这也是卷积神经网络模型强大的地方。

**2.2 池化层（Pooling layer）**

通常在卷积层之后会得到维度很大的特征，将特征切成几个区域，取其最大值或平均值，得到新的、维度较小的特征。

池化即下采样，目的是为了减少特征图，本质上其实就是对数据进行一个缩小。因为我们知道，比如人脸识别，通过卷积操作得到成千上万个feature map，每个feature map也有很多的像素点，这些对于后续的运算的时间会变得很长。池化其实就是对每个feature map进一步提炼的过程。如下图所示，原来4X4的feature map经过池化操作之后就变成了更小的2*2的矩阵。池化的方法包括max pooling，即取最大值，以及average pooling，即取平均值等等。

池化操作对每个深度切片独立，规模一般为 2＊2，相对于卷积层进行卷积运算，池化层进行的运算一般有以下几种：

- 最大池化（Max Pooling）。取4个点的最大值。这是最常用的池化方法。
- 均值池化（Mean Pooling）。取4个点的均值。
- 高斯池化。借鉴高斯模糊的方法。不常用。
- 可训练池化。训练函数 ff ，接受4个点为输入，出入1个点。不常用。

最常见的池化层是规模为2*2， 步幅为2，对输入的每个深度切片进行下采样。每个MAX操作对四个数进行，如下图所示：

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsFtXNXWGT70mokV6vkgUD9jvkzAUIGCEXkgfpEkOjJlCWngvYoTgetKrC94DBMFD3Lue09h5KeIAQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

池化操作将保存深度大小不变。如果池化层的输入单元大小不是二的整数倍，一般采取边缘补零（zero-padding）的方式补成2的倍数，然后再池化。

**2.3 非线性激活函数**

神经的非线性激活化函数，用于增加网络的非线性分割能力，一般用Relu函数。同时也可以叫做Normalization，就是将矩阵中负数的值转成0,也就是使用ReLu的激活函数进行负数变为0的操作。ReLu函数本质上就是max（0，x）。这一步其实也是为了方便运算。

**2.4 全连接层（ Fully-Connected layer）**

完全连接层是一个传统的多层感知器，它在输出层使用 softmax 激活函数。把所有局部特征结合变成全局特征，用来计算最后每一类的得分。卷积、ReLu、pooling，不断重复其实也就基本上构成了卷积神经网络的框架。一个卷积神经网络各层应用实例：

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsFtXNXWGT70mokV6vkgUD9jbPjwf35Z0iaH9bcpeEKJyG9yb6U5ZqRRCianXMvDprlz98M8H7aLWFrA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**CNN常见模型**

**1. 卷积神经网络基础：LeNet5**

手写字体识别模型LeNet5诞生于1994年，是最早的卷积神经网络之一。LeNet5通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个网络也是最近大量神经网络架构的起点。

如下图所示为LeNet网络结构，总共有7层网络（不含输入层），2个卷积层、2个池化层、3个全连接层。

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsH5zh1sRg16h4kXf7ibOXIUDVumhwGnst6vJZqkpSuORq02as8WfeDgSxMicLr6cMlxL4tEuWoib8icBA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用5*5的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为2*2，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。

卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。

在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层则将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10。

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsH5zh1sRg16h4kXf7ibOXIUDDRkdnjeIXDibcfdBglsA0lbT46bfx5qyJtVBQ9QEciaFSG3EWUkaib9ug/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1&retryload=1)

通过多次卷积和池化，CNN的最后一层将输入的图像像素映射为具体的输出。如在分类任务中会转换为不同类别的概率输出，然后计算真实标签与CNN模型的预测结果的差异，并通过反向传播更新每层的参数，并在更新完成后再次前向传播，如此反复直到训练完成 。 

一个数字识别的效果如图所示：

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsH5zh1sRg16h4kXf7ibOXIUDEv4OY9mBUu8Q2LkIKgYI9mYDsySVYLfuuKlFb6bvdg0wjUaFC7o70g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**2. 卷积神经网络进阶**

随着网络结构的发展，研究人员最初发现网络模型结构越深、网络参数越多模型的精度更优。比较典型的是AlexNet、VGG、InceptionV3和ResNet的发展脉络。  

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsH5zh1sRg16h4kXf7ibOXIUDpHicEBMqsXYBxkvWMnvPw77DtwL5JKqyaiaFZ5ConwGicT6SAKCOiaBiacw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**2.1 AlexNet(2012)**

2012年，AlexNet横空出世。这个模型的名字来源于论文第一作者的姓名Alex Krizhevsky。AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的现状。

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsH5zh1sRg16h4kXf7ibOXIUD9WSG9oj4BmGQGwwVYP0tmaG4CRaKWtfeQ6QoXfEFjbwZP64YUBjicVQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。AlexNet在LeNet的基础上增加了3个卷积层。但AlexNet作者对它们的卷积窗口、输出通道数和构造顺序均做了大量的调整，通过丢弃法来控制全连接层的模型复杂度；将sigmoid激活函数改成了更加简单的ReLU激活函数；引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。虽然AlexNet指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则以指导后来的研究者如何设计新的网络。

**2.2 VGG-16(2014)** 

VGG，它的名字来源于论文作者所在的实验室Visual Geometry Group。VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。VGG16相比AlexNet的一个改进是采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5），通过重复使用简单的基础块来构建深度模型的思路 。VGG16包含了16个隐藏层（13个卷积层和3个全连接层）。**VGG的结构图如下：**

![img](https://mmbiz.qpic.cn/mmbiz_jpg/vI9nYe94fsE7aLMjp0ernTUbEhpRE8WIeVibyyyrAANywOj7IhOyudBC02OulVxqLhKRL5ykaOSyiaXGVzZqm3GA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**VGG块的组成规律是**：连续使用数个相同的填充为1、窗口形状为3*3的卷积层后接上一个步幅为2、窗口形状为2*2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用**vgg_block**函数来实现这个基础的VGG块，它可以指定卷积层的数量和输入输出通道数。

> 对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。例如，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。

与AlexNet和LeNet一样，VGG网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个vgg_block，其超参数由变量conv_arch定义。该变量指定了每个VGG块里卷积层个数和输入输出通道数。全连接模块则跟AlexNet中的一样。

现在我们构造一个VGG网络。它有5个卷积块，前2块使用单卷积层，而后3块使用双卷积层。第一块的输入输出通道分别是1（因为下面要使用的Fashion-MNIST数据的通道数为1）和64，之后每次对输出通道数翻倍，直到变为512。因为这个网络使用了8个卷积层和3个全连接层，所以经常被称为VGG-11。

可以看到，每次我们将输入的高和宽减半，直到最终高和宽变成7后传入全连接层。与此同时，输出通道数每次翻倍，直到变成512。因为每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽、输入通道数和输出通道数的乘积成正比。VGG这种高和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。

VGG：通过重复使⽤简单的基础块来构建深度模型。  Block: 数个相同的填充为1、窗口形状为3×3的卷积层,接上一个步幅为2、窗口形状为2×2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。VGG和AlexNet的网络图对比如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsH5zh1sRg16h4kXf7ibOXIUDntJ234bhJGJh4xGod3qWGw5eRvX7CYT7gopZhHDtSliaKe45xRIk7Fw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsH5zh1sRg16h4kXf7ibOXIUDzBjYOvL9SQP2fGwube16uxyozyb02YDR99UnO5zvG8Kiac0xB7WujZw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)**小结**：VGG-11通过5个可以重复使用的卷积块来构造网络。根据每块里卷积层个数和输出通道数的不同可以定义出不同的VGG模型。   

**2.3 网络中的网络（NiN）**

LeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。NiN：串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。 ⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接用于分类。 

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsH5zh1sRg16h4kXf7ibOXIUDSGwVrpSiaM6ibB5AuNKXXaGv8ZiaW8HmqvCqmnRicrQaOIZakqCFicNRPiaQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**2.4 含并行连结的网络（GoogLeNet）**

在2014年的ImageNet图像识别挑战赛中，一个名叫GoogLeNet的网络结构大放异彩。它虽然在名字上向LeNet致敬，但在网络结构上已经很难看到LeNet的影子。GoogLeNet吸收了NiN中网络串联网络的思想，并在此基础上做了很大改进。在随后的几年里，研究人员对GoogLeNet进行了数次改进，本节将介绍这个模型系列的第一个版本。

- 由Inception基础块组成。 
- Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。 
- 可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 

Inception块GoogLeNet中的基础卷积块叫作Inception块，得名于同名电影《盗梦空间》（Inception）。与上一节的NiN块相比，这个基础块在结构上更加复杂。

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsH5zh1sRg16h4kXf7ibOXIUDpahFMicaD1N45HqXqcybG544fuj0SrCiaoYSTaZCkRcXibQJNI5VJhBFA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**2.5 残差网络（ResNet-50）**    

深度学习的问题：深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，准确率也变得更差。- - -残差块（Residual Block）恒等映射：

- 左边：f(x)=x；
- 右边：f(x)-x=0 （易于捕捉恒等映射的细微波动）。

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsH5zh1sRg16h4kXf7ibOXIUDGOHRcsQhsticS1Pobj9UiciciciaQqs6PaUIibeGKSwZbnjNKbpRlicBApjicA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的7*7卷积层后接步幅为2的3*3的最大池化层。不同之处在于ResNet每个卷积层后增加的批量归一化层。ResNet-50网络结构如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsH5zh1sRg16h4kXf7ibOXIUDoxrPysUiaq6mm9jv1CgIYkqyoHHjv2rmOY1xqVUhQJoEQum5Nicvp4CQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)