

## 随机梯度下降的一些改进方法

随机梯度下降法本质上是采用迭代方式更新参数，每次迭代在当前位置的基础上，沿着某一方向迈一小步抵达下一位置，然后在下一位置重复上述步骤。随机梯度下降法的更新公式表示为



![img](https://file.ai100.com.cn/files/sogou-articles/original/7340a1f1-fea7-4c18-af21-d6650ea93bc8/7340a1f1-fea7-4c18-af21-d6650ea93bc8)



其中，当前估计的负梯度−gt 表示步子的方向，学习速率η 控制步幅。改造的随机梯度下降法仍然基于这个更新公式。

**动量（Momentum）方法**

为了解决随机梯度下降法山谷震荡和鞍点停滞的问题，我们做一个简单的思维实验。想象一下纸团在山谷和鞍点处的运动轨迹，在山谷中纸团受重力作用沿山道滚下，两边是不规则的山壁，纸团不可避免地撞在山壁，由于质量小受山壁弹力的干扰大，从一侧山壁反弹回来撞向另一侧山壁，结果来回震荡地滚下；如果当纸团来到鞍点的一片平坦之地时，还是由于质量小，速度很快减为零。纸团的情况和随机梯度下降法遇到的问题简直如出一辙。直观地，如果换成一个铁球，当沿山谷滚下时，不容易受到途中旁力的干扰，轨迹会更稳更直；当来到鞍点中心处，在惯性作用下继续前行，从而有机会冲出这片平坦的陷阱。因此，有了动量方法，模型参数的迭代公式为



![img](https://file.ai100.com.cn/files/sogou-articles/original/3cbbdb65-15eb-4e5c-a3a7-385aceca0d39/3cbbdb65-15eb-4e5c-a3a7-385aceca0d39)



具体来说，前进步伐−vt由两部分组成。一是学习速率η乘以当前估计的梯度gt；二是带衰减的前一次步伐vt−1。这里，惯性就体现在对前一次步伐信息的重利用上。类比中学物理知识，当前梯度就好比当前时刻受力产生的加速度，前一次步伐好比前一时刻的速度，当前步伐好比当前时刻的速度。为了计算当前时刻的速度，应当考虑前一时刻速度和当前加速度共同作用的结果，因此vt直接依赖于vt−1和gt，而不仅仅是gt。另外，衰减系数γ扮演了阻力的作用。

中学物理还告诉我们，刻画惯性的物理量是动量，这也是算法名字的由来。沿山谷滚下的铁球，会受到沿坡道向下的力和与左右山壁碰撞的弹力。向下的力稳定不变，产生的动量不断累积，速度越来越快；左右的弹力总是在不停切换，动量累积的结果是相互抵消，自然减弱了球的来回震荡。因此，与随机梯度下降法相比，动量方法的收敛速度更快，收敛曲线也更稳定，如图7.5 所示。



![img](https://file.ai100.com.cn/files/sogou-articles/original/e4d4e36a-91a4-47ba-9be5-fa6d682ec46f/e4d4e36a-91a4-47ba-9be5-fa6d682ec46f)



**AdaGrad 方法**

惯性的获得是基于历史信息的，那么，除了从过去的步伐中获得一股子向前冲的劲儿，还能获得什么呢？我们还期待获得对周围环境的感知，即使蒙上双眼，依靠前几次迈步的感觉，也应该能判断出一些信息，比如这个方向总是坑坑洼洼的，那个方向可能很平坦。

随机梯度下降法对环境的感知是指在参数空间中，根据不同参数的一些经验性判断，自适应地确定参数的学习速率，不同参数的更新步幅是不同的。例如，在文本处理中训练词嵌入模型的参数时，有的词或词组频繁出现，有的词或词组则极少出现。数据的稀疏性导致相应参数的梯度的稀疏性，不频繁出现的词或词组的参数的梯度在大多数情况下为零，从而这些参数被更新的频率很低。在应用中，我们希望更新频率低的参数可以拥有较大的更新步幅，而更新频率高的参数的步幅可以减小。

AdaGrad 方法采用“历史梯度平方和”来衡量不同参数的梯度的稀疏性，取值越小表明越稀疏，具体的更新公式表示为



![img](https://file.ai100.com.cn/files/sogou-articles/original/4cb00e9a-1fc3-4756-8cef-f1066c2df4a4/4cb00e9a-1fc3-4756-8cef-f1066c2df4a4)



其中θt+1,i 表示（t+1）时刻的参数向量θt+1的第i个参数，gk,i表示k时刻的梯度向量gk 的第i 个维度（方向）。另外，分母中求和的形式实现了退火过程，这是很多优化技术中常见的策略，意味着随着时间推移，学习速率越

![img](https://file.ai100.com.cn/files/sogou-articles/original/1e749a1f-0f13-4eb2-bd0d-a9062e842c2d/1e749a1f-0f13-4eb2-bd0d-a9062e842c2d)来越小，从而保证了算法的最终收敛。



**Adam 方法**

Adam 方法将惯性保持和环境感知这两个优点集于一身。一方面， Adam 记录梯度的一阶矩（first moment），即过往梯度与当前梯度的平均，这体现了惯性保持；另一方面，Adam 还记录梯度的二阶矩（second moment），即过往梯度平方与当前梯度平方的平均，这类似AdaGrad 方法，体现了环境感知能力，为不同参数产生自适应的学习速率。一阶矩和二阶矩采用类似于滑动窗口内求平均的思想进行融合，即当前梯度和近一段时间内梯度的平均值，时间久远的梯度对当前平均值的贡献呈指数衰减。具体来说，一阶矩和二阶矩采用指数衰退平均（exponential decayaverage）技术，计算公式为



![img](https://file.ai100.com.cn/files/sogou-articles/original/96b69906-fc26-45c3-a44a-e90ef56c4f7f/96b69906-fc26-45c3-a44a-e90ef56c4f7f)



其中β1，β2 为衰减系数，mt 是一阶矩，vt 是二阶矩。

如何理解一阶矩和二阶矩呢？一阶矩相当于估计![img](https://file.ai100.com.cn/files/sogou-articles/original/2a90516a-ebf0-47ce-9f6a-9cfcb2e685b2/2a90516a-ebf0-47ce-9f6a-9cfcb2e685b2)：由于当下梯度gt 是随机采样得到的估计结果，因此更关注它在统计意义上的期望；二阶矩相当于估计![img](https://file.ai100.com.cn/files/sogou-articles/original/b4a5b18e-c69b-4af0-97ce-d047553a383f/b4a5b18e-c69b-4af0-97ce-d047553a383f)，这点与AdaGrad 方法不同，不是gt2从开始到现在的加和，而是它的期望。它们的物理意义是，当||mt||大且vt 大时，梯度大且稳定，这表明遇到一个明显的大坡，前进方向明确；当||mt||趋于零且vt大时，梯度不稳定，表明可能遇到一个峡谷，容易引起反弹震荡；当||mt||大且vt趋于零时，这种情况不可能出现；当||mt|| 趋于零且vt 趋于零时，梯度趋于零，可能到达局部最低点，也可能走到一片坡度极缓的平地，此时要避免陷入平原（plateau）。另外，Adam 方法还考虑了mt，vt 在零初始值情况下的偏置矫正。具体来说，Adam 的更新公式为



![img](https://file.ai100.com.cn/files/sogou-articles/original/98dadf37-f678-4933-a23f-5b382d7eec17/98dadf37-f678-4933-a23f-5b382d7eec17)



其中,![img](https://file.ai100.com.cn/files/sogou-articles/original/1890af51-37e6-463b-9857-a10b338bc172/1890af51-37e6-463b-9857-a10b338bc172)